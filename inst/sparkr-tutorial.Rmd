---
title: "SparkR"
output: 
  learnr::tutorial:
  progressive: true
allow_skip: false
runtime: shiny_prerendered
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objectifs

Cette formation va reprendre les exemples des tutoriaux pyspark sur parquet et hivemetastore.
Il est fortement recommandé de parcourir les formations précedentes en python (pyspark). Les concepts liés à Spark sont indépendant du langage pour "appeler" Spark. 

L'objectif est donc de voir comment avoir recours à Spark depuis R grace à l'API SparkR. 
Une librairie d'accès concurrente existe "Sparklyr"

Je prefère présenter la lirairie d'accès proposer par le projet Apache Spark.

## Initialisation du contexte Spark

Lors d'une phase de developpement sur donnée volumineuse, il est interessant d'utiliser une configuration dynamique du nombre d'executeurs afin de relacher les ressources lorsque l'on execute pas de taches.

```{r, exercise=TRUE}
df <- read.df("s3a://projet-spark-lab/diffusion/formation/data/sirene/sirene.csv", "csv", header = "true", inferSchema = "true")
```

```{r, exercise=TRUE}
head(df)
```
